{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccbe957d-41bf-43e2-8265-49027ccd7175",
   "metadata": {},
   "source": [
    "# Prérequis\n",
    "0. Installation des libairies necessaire pour le développement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac8c277-42eb-4122-920f-32e70c9c5e3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b0c55-7cd0-43eb-a60a-94dbcf8e657a",
   "metadata": {},
   "source": [
    "## Récupération d'un ensemble de données d'IRM cérébrale\n",
    "Source : https://www.kaggle.com/datasets/navoneel/brain-mri-images-for-brain-tumor-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4702d21a-808e-485b-b326-0badf3154cfd",
   "metadata": {},
   "source": [
    "# Importation des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc4671-7a0d-490e-b8e1-8857b781ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import random\n",
    "import cv2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb393dc-8418-4375-ac3c-c21da60997ec",
   "metadata": {},
   "source": [
    "## Lire les images IRM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffaac50-7085-4c08-927a-5ac61e7e0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(directory, img_size=(128, 128)):\n",
    "    images = []\n",
    "    path = f'{directory}/*.[jJ][pP][gG]'\n",
    "    \n",
    "    for file in glob.iglob(path):\n",
    "        try:\n",
    "            # Read and resize image\n",
    "            img = cv2.imread(file)\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read image {file}\")\n",
    "                continue\n",
    "                \n",
    "            img = cv2.resize(img, img_size)\n",
    "            \n",
    "            # Convert BGR to RGB (OpenCV loads as BGR by default)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Load tumor and healthy brain images\n",
    "tumor = load_images('data/brain_tumor_dataset/yes')\n",
    "healthy = load_images('data/brain_tumor_dataset/no')\n",
    "\n",
    "print(f\"Loaded {len(tumor)} tumor images and {len(healthy)} healthy images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5776d-2155-4a60-ae21-46d5e7053ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor = np.array(tumor)\n",
    "healthy = np.array(healthy)\n",
    "\n",
    "tumor_and_healthy = np.concatenate((healthy, tumor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e454d8d-5838-4cb4-b190-acfef181edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy.shape\n",
    "# (amount_of_files, width, height, channel) -> Each channel has a width and height of 128x128 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a55a15-0082-433e-8016-93d6795e319a",
   "metadata": {},
   "source": [
    "## Visualiser les images IRM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38808ee6-e48a-448d-ac5f-3173187f9225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random(healthy, tumor, num=5):\n",
    "    # This generates 5(num) numbers from 0 to 96 with no dublicate\n",
    "    healthy_imgs = healthy[np.random.choice(healthy.shape[0], num, replace=False)]\n",
    "    tumor_imgs = tumor[np.random.choice(tumor.shape[0], num, replace=False)]\n",
    "\n",
    "    # Displaying healthy images\n",
    "    plt.figure(figsize=(16,9))\n",
    "    for i in range(num):\n",
    "        plt.subplot(1, num, i+1)\n",
    "        plt.title('Healthy')\n",
    "        plt.imshow(healthy_imgs[i])\n",
    "\n",
    "    # Displaying images with tumors\n",
    "    plt.figure(figsize=(16,9))\n",
    "    for i in range(num):\n",
    "        plt.subplot(1, num, i+1)\n",
    "        plt.title('Tumor')\n",
    "        plt.imshow(tumor_imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07031d74-1d8a-4da9-9522-94cc3b8273dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random(healthy, tumor, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0739a6-d77a-47f5-a322-6ff3e78c8e49",
   "metadata": {},
   "source": [
    "## La class Dataset de PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaab1a6-a4a7-4329-b095-937ecf9b6152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    \"\"\"Cette class est une class abstraite representant un Dataset\n",
    "\n",
    "    Toute autre class de dataset devrait etre une sous class de celle-ci.\n",
    "    Et chaque class devrait 'Ecraser' ``__len__``, qui retourne la taille du dataset, et\n",
    "    ``__getitem__``, qui supporte les index en entier qui va de 0 a len(self) exclusive.\n",
    "    \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21d655-4fd6-476d-a9ca-decea26cac30",
   "metadata": {},
   "source": [
    "## Creation de la class IRM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece7bbbf-d320-4a5d-9d24-ca32721d384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRM(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        tumor = []\n",
    "        healthy = []\n",
    "        # cv2 - It reads in BGR format by default\n",
    "        for f in glob.iglob(\"./data/brain_tumor_dataset/yes/*.jpg\"):\n",
    "            img = cv2.imread(f)\n",
    "            img = cv2.resize(img,(128,128)) \n",
    "            b, g, r = cv2.split(img)\n",
    "            img = cv2.merge([r,g,b])\n",
    "            img = img.reshape((img.shape[2],img.shape[0],img.shape[1]))\n",
    "            tumor.append(img)\n",
    "\n",
    "        for f in glob.iglob(\"./data/brain_tumor_dataset/no/*.jpg\"):\n",
    "            img = cv2.imread(f)\n",
    "            img = cv2.resize(img,(128,128)) \n",
    "            b, g, r = cv2.split(img)\n",
    "            img = cv2.merge([r,g,b])\n",
    "            img = img.reshape((img.shape[2],img.shape[0],img.shape[1]))\n",
    "            healthy.append(img)\n",
    "\n",
    "        # Nos images\n",
    "        tumor = np.array(tumor,dtype=np.float32)\n",
    "        healthy = np.array(healthy,dtype=np.float32)\n",
    "        \n",
    "        # Nos titres\n",
    "        tumor_label = np.ones(tumor.shape[0], dtype=np.float32)\n",
    "        healthy_label = np.zeros(healthy.shape[0], dtype=np.float32)\n",
    "        \n",
    "        # Concatenation des deux\n",
    "        self.images = np.concatenate((tumor, healthy), axis=0)\n",
    "        self.labels = np.concatenate((tumor_label, healthy_label))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sample = {'image': self.images[index], 'label':self.labels[index]}\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def normalize(self):\n",
    "        self.images = self.images/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd86d27-8e74-499c-8a46-1fa226bd22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "irm = IRM()\n",
    "irm.normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0575b23-8894-4ce8-a904-57762b1e8cfe",
   "metadata": {},
   "source": [
    "# Extraction des données (DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a3ea0f-4190-4fe5-8391-8d97f1cc8313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = list(range(len(irm)))\n",
    "random.shuffle(index)\n",
    "\n",
    "for i in index:\n",
    "    sample = irm[i]\n",
    "    img = sample['image']\n",
    "    label = sample['label']\n",
    "    img = img.reshape(img.shape[1], img.shape[2], img.shape[0])\n",
    "    plt.title(label)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8575bf8-6649-40c5-92c5-496141a5281b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "it = iter(irm)\n",
    "for i in range(10):\n",
    "    sample = next(it)\n",
    "    img = sample['image']\n",
    "    label = sample['label']\n",
    "    img = img.reshape(img.shape[1], img.shape[2], img.shape[0])\n",
    "    plt.title(label)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d691c99-d59c-4da1-95a3-41ff16a3eade",
   "metadata": {},
   "source": [
    "## Utilisation du DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e45f6c6-3574-4944-924f-94a5ebfc910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size c'est pour avoir plusieur image dans un 'batch' : \n",
    "# - torch.Size([10, 3, 128, 128]): c'est a dire que chaque iteration on a 10 images en une fois\n",
    "# shuggle c'est pour mixer les image (tumeur, sans tumeur)\n",
    "dataloader = DataLoader(irm, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ffc0d-12f9-44ae-89b2-943c7854bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dataloader:\n",
    "    img = sample['image']\n",
    "    print(img.shape)\n",
    "    #img = img.reshape(img.shape[1], img.shape[2], img.shape[0])\n",
    "    #plt.imshow(img)\n",
    "    #plt.show()\n",
    "    #print(img.shape)\n",
    "    #sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea862c-e2f7-4e56-ac33-7905e9a20789",
   "metadata": {},
   "source": [
    "## Creation du CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4051f3ca-13dc-4189-b7ac-4b95758df1b6",
   "metadata": {},
   "source": [
    "$$\n",
    "n_{\\text{out}} = \\lfloor \\frac{n_{\\text{in}} + 2p - f}{s} + 1 \\rfloor\n",
    "$$\n",
    "- $f$ = kernel_size\n",
    "- $s$ = stride\n",
    "- $p$ = padding\n",
    "- $n_{in}$ = dimension of the input data (which is the output of the previous layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefa84d1-e7d2-429a-963e-1965190100e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch veux qu'on herite de nn.Module (une sous classe)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn_model = nn.Sequential(\n",
    "            # Premier couche convolutive (LOW LEVEL)\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5), \n",
    "            nn.Tanh(), # ca permet de transformer nos données entre [-1, 1]\n",
    "            nn.AvgPool2d(kernel_size=2, stride=5, padding=0),\n",
    "            # Deuxieme couche conv. (Mid-Level)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=5, padding=0)\n",
    "        )\n",
    "\n",
    "        self.fc_model = nn.Sequential(\n",
    "            nn.Linear(in_features=256, out_features=120),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=120, out_features=84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=84, out_features=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_model(x)\n",
    "        x = x.view(x.size(0), -1) # applati les 2D array\n",
    "        x = self.fc_model(x)\n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e565d2ee-f427-4cd0-b5f8-dcf99426a32e",
   "metadata": {},
   "source": [
    "## Analyse des parametres du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e09e0-99c0-4772-8179-390dd045d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527c780-4c47-4202-929a-afc5c0578c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912c530-beb3-4241-b6a6-4f83e6b8beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b54e6ec-892b-473a-8be1-919d5faebefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cnn_model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb32dcf-a30e-442d-8edb-a17e062f83d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cnn_model[0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9a553-2c8e-4e04-a567-7e736c8f5397",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cnn_model[0].weight[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925eca4b-0b38-44f4-8569-4e54e61ea01a",
   "metadata": {},
   "source": [
    "## Couche linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f46b5c8-2a5e-41ac-a1df-684b3317b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc_model[0].weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df9761a-86af-4dc7-9e1e-cc3212c029b6",
   "metadata": {},
   "source": [
    "## Explication de x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4d2df-f311-4aae-a37a-fc99f677b084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16])\n",
    "x = x.reshape((2,2,2,2))\n",
    "x.size() # retourne (2,2,2,2)\n",
    "x.size(0) # return 2\n",
    "x.view(-1) # tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n",
    "y = x.view(x.size(0), -1) # tensor([[ 1,  2,  3,  4,  5,  6,  7,  8],\n",
    "                      # [ 9, 10, 11, 12, 13, 14, 15, 16]])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70c95e-a574-4fde-8106-6a5b9c4d5c96",
   "metadata": {},
   "source": [
    "## torche.testor vs. torch.cuda.tensor\n",
    "### Les tensor sur le CPU sont pas de meme type que les tensor sur GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f9c196-36ed-441e-b8fa-446067e32078",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    'mps' if torch.backends.mps.is_available() else # Apple \n",
    "    'cuda' if torch.cuda.is_available() else # Nvidia\n",
    "    'cpu'\n",
    ")\n",
    "\n",
    "cpu_tensor = torch.rand(10)\n",
    "gpu_tensor = torch.rand(10).to(device)\n",
    "\n",
    "print(cpu_tensor, cpu_tensor.dtype, type(cpu_tensor), cpu_tensor.type())\n",
    "print(gpu_tensor, gpu_tensor.dtype, type(gpu_tensor), gpu_tensor.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c2b355-6c82-4c68-9590-3b2032099ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir un tensor -> numpy array\n",
    "gpu_tensor.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edbacfc-0981-42e6-b017-fe66ae8f12d9",
   "metadata": {},
   "source": [
    "## Test CNN (sans entrainement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c0d587-9dc4-46c2-8e03-36cada3359c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "irm_dataset = IRM()\n",
    "irm_dataset.normalize()\n",
    "device = torch.device(\n",
    "    'mps' if torch.backends.mps.is_available() else # Apple \n",
    "    'cuda' if torch.cuda.is_available() else # Nvidia\n",
    "    'cpu'\n",
    ")\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08885df3-d3d3-436f-9cb1-45ac23270cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(irm_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b42cea-03de-49ca-8074-ff4b50c90412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval - Deactive le dropout (pour que pas tout les neuronnes sont actives)\n",
    "model.eval()\n",
    "output = []\n",
    "y_true = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for D in dataloader:\n",
    "        image = D['image'].to(device)\n",
    "        label = D['label'].to(device)\n",
    "    \n",
    "        y_hat = model(image)\n",
    "    \n",
    "        output.append(y_hat.cpu().detach().numpy())\n",
    "        y_true.append(label.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5cc02-37a6-41d0-b77a-af396c7a34fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.concatenate(output, axis=0).squeeze()\n",
    "y_true = np.concatenate(y_true, axis=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4668e9-5fbf-4af9-b435-38b31da74563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(scores, threshold=0.50, minimum=0, maximum=1.0):\n",
    "    x = np.array(list(scores))\n",
    "    x[x >= threshold] = maximum\n",
    "    x[x < threshold] = minimum\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77cae4d-f5cc-49e6-9327-f044b8435368",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, threshold(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a23896b-c4a7-4376-a209-a5f49bcc3738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "cm = confusion_matrix(y_true, threshold(output))\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax, annot_kws={\"size\":20})\n",
    "\n",
    "ax.set_xlabel('Predicted labels', fontsize=20)\n",
    "ax.set_ylabel('True labels', fontsize=20)\n",
    "ax.set_title('Confusion Matrix', fontsize=20)\n",
    "ax.xaxis.set_ticklabels(['Healthy', 'Tumor'], fontsize=20)\n",
    "ax.yaxis.set_ticklabels(['Tumor', 'Healthy'], fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b236a542-30d5-47b8-849a-5fa2b3a255b3",
   "metadata": {},
   "source": [
    "## Entrainer le model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e249326d-3fa5-4edd-9406-d56f6ce07f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.0001\n",
    "EPOCH = 400\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=eta)\n",
    "dataloader = DataLoader(irm_dataset, batch_size=32, shuffle=True)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d25c4-b180-4a7a-b8bf-c99623c62d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCH):\n",
    "    losses = []\n",
    "    for D in dataloader:\n",
    "\n",
    "        optimizer.zero_grad() # Important\n",
    "        \n",
    "        data = D['image'].to(device)\n",
    "        label = D['label'].to(device)\n",
    "        y_hat = model(data)\n",
    "\n",
    "        # Definir la fonction de perte (loss)\n",
    "        error = nn.BCELoss()\n",
    "        loss = torch.sum(error(y_hat.squeeze(), label))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('Train Epoch: {} Loss: {:.6f}'.format(epoch+1, np.mean(losses)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ae936-37e4-4512-877b-8c576b346992",
   "metadata": {},
   "source": [
    "## Evaluation du model après entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55dd381-dc20-4106-afed-847cde9ce863",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dataloader = DataLoader(irm_dataset, batch_size=32, shuffle=False)\n",
    "output = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for D in dataloader:\n",
    "        image = D['image'].to(device)\n",
    "        label = D['label'].to(device)\n",
    "\n",
    "        y_hat = model(image)\n",
    "\n",
    "        output.append(y_hat.cpu().detach().numpy())\n",
    "        y_true.append(label.cpu().detach().numpy())\n",
    "\n",
    "output = np.concatenate(output, axis=0)\n",
    "y_true = np.concatenate(y_true, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9fed06-d2b9-4195-b66b-326facc61bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, threshold(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e56bbcf-4867-438c-98da-45af2d62f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "cm = confusion_matrix(y_true, threshold(output))\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax, annot_kws={\"size\":20})\n",
    "\n",
    "ax.set_xlabel('Predicted labels', fontsize=20)\n",
    "ax.set_ylabel('True labels', fontsize=20)\n",
    "ax.set_title('Confusion Matrix', fontsize=20)\n",
    "ax.xaxis.set_ticklabels(['Healthy', 'Tumor'], fontsize=20)\n",
    "ax.yaxis.set_ticklabels(['Tumor', 'Healthy'], fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa866a10-4cd6-49c3-a1da-ad250eb1b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(output)\n",
    "plt.axvline(x=len(tumor), color='r', linestyle='--')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d62c53-08ff-4baa-b6dc-406b135310ac",
   "metadata": {},
   "source": [
    "## Visualiser utilisant une 'Feature Map'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1c20a-ef9b-4524-bae5-5925356f0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655211e-04f8-4c7c-8438-bc4bf0d298fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_layer = 0\n",
    "conv_layers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f8e4bf-daeb-4373-9ae6-564d38a7fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_children = list(model.children())\n",
    "model_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e4296f-62b7-41ff-9d6b-2fca23b1224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in model_children:\n",
    "    if type(child) == nn.Sequential:\n",
    "        for layer in child.children():\n",
    "            if type(layer) == nn.Conv2d:\n",
    "                no_of_layer += 1\n",
    "                conv_layers.append(layer)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa11d6fd-46fb-4645-b8d3-7d3be21e439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd686d-fb4d-4452-a0dc-e7fa9f72fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = irm_dataset[100]['image']\n",
    "plt.imshow(img.reshape(128,128,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9716ec5-120c-4e8d-88fc-43f9b55a226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.from_numpy(img).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1461775b-80c3-42e5-89fa-f6b168ba900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f61eed-9697-48ee-9a10-4ae9b092b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.unsqueeze(0)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf553cb9-7be3-4397-81df-7f32d051dfe1",
   "metadata": {},
   "source": [
    "## Feature Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb46f602-8b34-4194-adb5-2810df87123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [conv_layers[0](img)]\n",
    "for i in range(1, len(conv_layers)):\n",
    "    results.append(conv_layers[i](results[-1]))\n",
    "output = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f0eee-c896-4f7f-a178-bbd8530e0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e26a1-0353-48ad-9e7d-ffb7b245e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0ec15b-3d97-47dc-8330-244a1e0128f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num_layer in range(len(output)):\n",
    "    plt.figure(figsize=(50,10))\n",
    "    layer_viz = output[num_layer].squeeze()\n",
    "    print(\"Layer \", num_layer+1)\n",
    "    for i, f in enumerate(layer_viz):\n",
    "        plt.subplot(2, 8, i + 1)\n",
    "        plt.imshow(f.detach().cpu().numpy())\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1814af98-6c74-4a92-a12b-d7ffaaf6edfb",
   "metadata": {},
   "source": [
    "## GRAD-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b544e-6679-4f61-a564-e02a16ad1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (BTD)",
   "language": "python",
   "name": "btd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
