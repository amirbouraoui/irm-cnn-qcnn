{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccbe957d-41bf-43e2-8265-49027ccd7175",
   "metadata": {},
   "source": [
    "# Prérequis\n",
    "0. Installation des libairies necessaire pour le développement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac8c277-42eb-4122-920f-32e70c9c5e3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qtconsole in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 88)) (5.6.1)\n",
      "Requirement already satisfied: QtPy in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 89)) (2.4.3)\n",
      "Requirement already satisfied: referencing in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 90)) (0.36.2)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 91)) (2.32.3)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 92)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 93)) (0.1.1)\n",
      "Requirement already satisfied: rpds-py in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 94)) (0.23.1)\n",
      "Requirement already satisfied: ruamel.yaml in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 95)) (0.18.10)\n",
      "Requirement already satisfied: ruamel.yaml.clib in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 96)) (0.2.12)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 97)) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 98)) (1.15.2)\n",
      "Requirement already satisfied: seaborn in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 99)) (0.13.2)\n",
      "Requirement already satisfied: Send2Trash in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 100)) (1.8.3)\n",
      "Requirement already satisfied: six in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 101)) (1.16.0)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 102)) (1.3.1)\n",
      "Requirement already satisfied: soupsieve in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 103)) (2.6)\n",
      "Requirement already satisfied: stack-data in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 104)) (0.6.3)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 105)) (1.13.1)\n",
      "Requirement already satisfied: terminado in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 106)) (0.18.1)\n",
      "Requirement already satisfied: threadpoolctl in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 107)) (3.6.0)\n",
      "Requirement already satisfied: tinycss2 in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 108)) (1.4.0)\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 109)) (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 110)) (0.21.0)\n",
      "Requirement already satisfied: tornado in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 111)) (6.4.2)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 112)) (4.67.1)\n",
      "Requirement already satisfied: traitlets in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 113)) (5.14.3)\n",
      "Requirement already satisfied: types-python-dateutil in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 114)) (2.9.0.20241206)\n",
      "Requirement already satisfied: typing_extensions in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 115)) (4.12.2)\n",
      "Requirement already satisfied: tzdata in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 116)) (2025.1)\n",
      "Requirement already satisfied: uri-template in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 117)) (1.3.0)\n",
      "Requirement already satisfied: urllib3 in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 118)) (2.3.0)\n",
      "Requirement already satisfied: wcwidth in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 119)) (0.2.13)\n",
      "Requirement already satisfied: webcolors in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 120)) (24.11.1)\n",
      "Requirement already satisfied: webencodings in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 121)) (0.5.1)\n",
      "Requirement already satisfied: websocket-client in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 122)) (1.8.0)\n",
      "Requirement already satisfied: widgetsnbextension in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 123)) (4.0.13)\n",
      "Requirement already satisfied: grad-cam in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from -r requirements.txt (line 124)) (1.5.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from anyio->-r requirements.txt (line 1)) (1.2.2)\n",
      "Requirement already satisfied: appnope in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 31)) (0.1.4)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 32)) (4.9.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from jupyterlab->-r requirements.txt (line 50)) (75.8.0)\n",
      "Requirement already satisfied: tomli>=1.2.2 in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from jupyterlab->-r requirements.txt (line 50)) (2.2.1)\n",
      "Requirement already satisfied: ptyprocess in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from terminado->-r requirements.txt (line 106)) (0.7.0)\n",
      "Requirement already satisfied: ttach in /opt/homebrew/Caskroom/miniforge/base/envs/BTD-3.10/lib/python3.10/site-packages (from grad-cam->-r requirements.txt (line 124)) (0.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b0c55-7cd0-43eb-a60a-94dbcf8e657a",
   "metadata": {},
   "source": [
    "## Récupération d'un ensemble de données d'IRM cérébrale\n",
    "Source : https://www.kaggle.com/datasets/navoneel/brain-mri-images-for-brain-tumor-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4702d21a-808e-485b-b326-0badf3154cfd",
   "metadata": {},
   "source": [
    "# Importation des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc4671-7a0d-490e-b8e1-8857b781ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import random\n",
    "import cv2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb393dc-8418-4375-ac3c-c21da60997ec",
   "metadata": {},
   "source": [
    "## Lire les images IRM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffaac50-7085-4c08-927a-5ac61e7e0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(directory, img_size=(128, 128)):\n",
    "    images = []\n",
    "    path = f'{directory}/*.[jJ][pP][gG]'\n",
    "    \n",
    "    for file in glob.iglob(path):\n",
    "        try:\n",
    "            # Read and resize image\n",
    "            img = cv2.imread(file)\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read image {file}\")\n",
    "                continue\n",
    "                \n",
    "            img = cv2.resize(img, img_size)\n",
    "            \n",
    "            # Convert BGR to RGB (OpenCV loads as BGR by default)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Load tumor and healthy brain images\n",
    "tumor = load_images('../data/brain_tumor_dataset/yes')\n",
    "healthy = load_images('../data/brain_tumor_dataset/no')\n",
    "\n",
    "print(f\"Loaded {len(tumor)} tumor images and {len(healthy)} healthy images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5776d-2155-4a60-ae21-46d5e7053ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor = np.array(tumor)\n",
    "healthy = np.array(healthy)\n",
    "\n",
    "tumor_and_healthy = np.concatenate((healthy, tumor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e454d8d-5838-4cb4-b190-acfef181edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy.shape\n",
    "# (amount_of_files, width, height, channel) -> Each channel has a width and height of 128x128 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a55a15-0082-433e-8016-93d6795e319a",
   "metadata": {},
   "source": [
    "## Visualiser les images IRM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38808ee6-e48a-448d-ac5f-3173187f9225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random(healthy, tumor, num=5):\n",
    "    # This generates 5(num) numbers from 0 to 96 with no dublicate\n",
    "    healthy_imgs = healthy[np.random.choice(healthy.shape[0], num, replace=False)]\n",
    "    tumor_imgs = tumor[np.random.choice(tumor.shape[0], num, replace=False)]\n",
    "\n",
    "    # Displaying healthy images\n",
    "    plt.figure(figsize=(16,9))\n",
    "    for i in range(num):\n",
    "        plt.subplot(1, num, i+1)\n",
    "        plt.title('Healthy')\n",
    "        plt.imshow(healthy_imgs[i])\n",
    "\n",
    "    # Displaying images with tumors\n",
    "    plt.figure(figsize=(16,9))\n",
    "    for i in range(num):\n",
    "        plt.subplot(1, num, i+1)\n",
    "        plt.title('Tumor')\n",
    "        plt.imshow(tumor_imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07031d74-1d8a-4da9-9522-94cc3b8273dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random(healthy, tumor, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0739a6-d77a-47f5-a322-6ff3e78c8e49",
   "metadata": {},
   "source": [
    "## La class Dataset de PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaab1a6-a4a7-4329-b095-937ecf9b6152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    \"\"\"Cette class est une class abstraite representant un Dataset\n",
    "\n",
    "    Toute autre class de dataset devrait etre une sous class de celle-ci.\n",
    "    Et chaque class devrait 'Ecraser' ``__len__``, qui retourne la taille du dataset, et\n",
    "    ``__getitem__``, qui supporte les index en entier qui va de 0 a len(self) exclusive.\n",
    "    \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21d655-4fd6-476d-a9ca-decea26cac30",
   "metadata": {},
   "source": [
    "## Creation de la class IRM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece7bbbf-d320-4a5d-9d24-ca32721d384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRM(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        tumor = []\n",
    "        healthy = []\n",
    "        # cv2 - It reads in BGR format by default\n",
    "        for f in glob.iglob(\"../data/brain_tumor_dataset/yes/*.jpg\"):\n",
    "            img = cv2.imread(f)\n",
    "            img = cv2.resize(img,(128,128)) \n",
    "            b, g, r = cv2.split(img)\n",
    "            img = cv2.merge([r,g,b])\n",
    "            img = img.reshape((img.shape[2],img.shape[0],img.shape[1]))\n",
    "            tumor.append(img)\n",
    "\n",
    "        for f in glob.iglob(\"../data/brain_tumor_dataset/no/*.jpg\"):\n",
    "            img = cv2.imread(f)\n",
    "            img = cv2.resize(img,(128,128)) \n",
    "            b, g, r = cv2.split(img)\n",
    "            img = cv2.merge([r,g,b])\n",
    "            img = img.reshape((img.shape[2],img.shape[0],img.shape[1]))\n",
    "            healthy.append(img)\n",
    "        \n",
    "        print(f\"Tumor count: {len(tumor)}, Healthy count: {len(healthy)}\")\n",
    "\n",
    "        # Nos images\n",
    "        tumor = np.array(tumor,dtype=np.float32)\n",
    "        healthy = np.array(healthy,dtype=np.float32)\n",
    "        \n",
    "        # Nos titres\n",
    "        tumor_label = np.ones(tumor.shape[0], dtype=np.float32)\n",
    "        healthy_label = np.zeros(healthy.shape[0], dtype=np.float32)\n",
    "        \n",
    "        # Concatenation des deux\n",
    "        self.images = np.concatenate((tumor, healthy), axis=0)\n",
    "        self.labels = np.concatenate((tumor_label, healthy_label))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sample = {'image': self.images[index], 'label':self.labels[index]}\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def normalize(self):\n",
    "        self.images = self.images/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd86d27-8e74-499c-8a46-1fa226bd22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "irm = IRM()\n",
    "irm.normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0575b23-8894-4ce8-a904-57762b1e8cfe",
   "metadata": {},
   "source": [
    "# Extraction des données (DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a3ea0f-4190-4fe5-8391-8d97f1cc8313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = list(range(len(irm)))\n",
    "random.shuffle(index)\n",
    "\n",
    "for i in index:\n",
    "    sample = irm[i]\n",
    "    img = sample['image']\n",
    "    label = sample['label']\n",
    "    img = img.reshape(img.shape[1], img.shape[2], img.shape[0])\n",
    "    plt.title(label)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8575bf8-6649-40c5-92c5-496141a5281b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "it = iter(irm)\n",
    "for i in range(10):\n",
    "    sample = next(it)\n",
    "    img = sample['image']\n",
    "    label = sample['label']\n",
    "    img = img.reshape(img.shape[1], img.shape[2], img.shape[0])\n",
    "    plt.title(label)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d691c99-d59c-4da1-95a3-41ff16a3eade",
   "metadata": {},
   "source": [
    "## Utilisation du DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e45f6c6-3574-4944-924f-94a5ebfc910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size c'est pour avoir plusieur image dans un 'batch' : \n",
    "# - torch.Size([10, 3, 128, 128]): c'est a dire que chaque iteration on a 10 images en une fois\n",
    "# shuggle c'est pour mixer les image (tumeur, sans tumeur)\n",
    "dataloader = DataLoader(irm, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ffc0d-12f9-44ae-89b2-943c7854bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dataloader:\n",
    "    img = sample['image']\n",
    "    print(img.shape)\n",
    "    #img = img.reshape(img.shape[1], img.shape[2], img.shape[0])\n",
    "    #plt.imshow(img)\n",
    "    #plt.show()\n",
    "    #print(img.shape)\n",
    "    #sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea862c-e2f7-4e56-ac33-7905e9a20789",
   "metadata": {},
   "source": [
    "## Creation du CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4051f3ca-13dc-4189-b7ac-4b95758df1b6",
   "metadata": {},
   "source": [
    "$$\n",
    "n_{\\text{out}} = \\lfloor \\frac{n_{\\text{in}} + 2p - f}{s} + 1 \\rfloor\n",
    "$$\n",
    "- $f$ = kernel_size\n",
    "- $s$ = stride\n",
    "- $p$ = padding\n",
    "- $n_{in}$ = dimension of the input data (which is the output of the previous layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefa84d1-e7d2-429a-963e-1965190100e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch veux qu'on herite de nn.Module (une sous classe)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn_model = nn.Sequential(\n",
    "            # Premier couche convolutive (LOW LEVEL)\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5), \n",
    "            nn.Tanh(), # ca permet de transformer nos données entre [-1, 1]\n",
    "            nn.AvgPool2d(kernel_size=2, stride=5, padding=0),\n",
    "            # Deuxieme couche conv. (Mid-Level)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=5, padding=0)\n",
    "        )\n",
    "\n",
    "        self.fc_model = nn.Sequential(\n",
    "            nn.Linear(in_features=256, out_features=120),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=120, out_features=84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=84, out_features=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_model(x)\n",
    "        x = x.view(x.size(0), -1) # applati les 2D array\n",
    "        x = self.fc_model(x)\n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba4401-482e-4538-93ca-5e3f828a65c1",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch veux qu'on herite de nn.Module (une sous classe)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.cnn_model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((4, 4))\n",
    "        )\n",
    "        \n",
    "        self.fc_model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_model(x)\n",
    "        x = self.fc_model(x)\n",
    "        x = torch.sigmoid(x) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e565d2ee-f427-4cd0-b5f8-dcf99426a32e",
   "metadata": {},
   "source": [
    "## Analyse des parametres du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e09e0-99c0-4772-8179-390dd045d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527c780-4c47-4202-929a-afc5c0578c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912c530-beb3-4241-b6a6-4f83e6b8beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b54e6ec-892b-473a-8be1-919d5faebefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cnn_model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb32dcf-a30e-442d-8edb-a17e062f83d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cnn_model[0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9a553-2c8e-4e04-a567-7e736c8f5397",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cnn_model[0].weight[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925eca4b-0b38-44f4-8569-4e54e61ea01a",
   "metadata": {},
   "source": [
    "## Couche linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e70c452-9097-4802-af43-7f650318523a",
   "metadata": {},
   "source": [
    "model.fc_model[0].weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df9761a-86af-4dc7-9e1e-cc3212c029b6",
   "metadata": {},
   "source": [
    "## Explication de x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4d2df-f311-4aae-a37a-fc99f677b084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16])\n",
    "x = x.reshape((2,2,2,2))\n",
    "x.size() # retourne (2,2,2,2)\n",
    "x.size(0) # return 2\n",
    "x.view(-1) # tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n",
    "y = x.view(x.size(0), -1) # tensor([[ 1,  2,  3,  4,  5,  6,  7,  8],\n",
    "                      # [ 9, 10, 11, 12, 13, 14, 15, 16]])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70c95e-a574-4fde-8106-6a5b9c4d5c96",
   "metadata": {},
   "source": [
    "## torche.testor vs. torch.cuda.tensor\n",
    "### Les tensor sur le CPU sont pas de meme type que les tensor sur GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f9c196-36ed-441e-b8fa-446067e32078",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    'mps' if torch.backends.mps.is_available() else # Apple \n",
    "    'cuda' if torch.cuda.is_available() else # Nvidia\n",
    "    'cpu'\n",
    ")\n",
    "\n",
    "cpu_tensor = torch.rand(10)\n",
    "gpu_tensor = torch.rand(10).to(device)\n",
    "\n",
    "print(cpu_tensor, cpu_tensor.dtype, type(cpu_tensor), cpu_tensor.type())\n",
    "print(gpu_tensor, gpu_tensor.dtype, type(gpu_tensor), gpu_tensor.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c2b355-6c82-4c68-9590-3b2032099ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir un tensor -> numpy array\n",
    "gpu_tensor.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edbacfc-0981-42e6-b017-fe66ae8f12d9",
   "metadata": {},
   "source": [
    "## Test CNN (sans entrainement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c0d587-9dc4-46c2-8e03-36cada3359c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "irm_dataset = IRM()\n",
    "irm_dataset.normalize()\n",
    "device = torch.device(\n",
    "    'mps' if torch.backends.mps.is_available() else # Apple \n",
    "    'cuda' if torch.cuda.is_available() else # Nvidia\n",
    "    'cpu'\n",
    ")\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08885df3-d3d3-436f-9cb1-45ac23270cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(irm_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b42cea-03de-49ca-8074-ff4b50c90412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval - Deactive le dropout (pour que pas tout les neuronnes sont actives)\n",
    "model.eval()\n",
    "output = []\n",
    "y_true = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for D in dataloader:\n",
    "        image = D['image'].to(device)\n",
    "        label = D['label'].to(device)\n",
    "    \n",
    "        y_hat = model(image)\n",
    "    \n",
    "        output.append(y_hat.cpu().detach().numpy())\n",
    "        y_true.append(label.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5cc02-37a6-41d0-b77a-af396c7a34fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.concatenate(output, axis=0).squeeze()\n",
    "y_true = np.concatenate(y_true, axis=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4668e9-5fbf-4af9-b435-38b31da74563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(scores, threshold=0.50, minimum=0, maximum=1.0):\n",
    "    x = np.array(list(scores))\n",
    "    x[x >= threshold] = maximum\n",
    "    x[x < threshold] = minimum\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77cae4d-f5cc-49e6-9327-f044b8435368",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, threshold(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a23896b-c4a7-4376-a209-a5f49bcc3738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "cm = confusion_matrix(y_true, threshold(output))\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax, annot_kws={\"size\":20})\n",
    "\n",
    "ax.set_xlabel('Predicted labels', fontsize=20)\n",
    "ax.set_ylabel('True labels', fontsize=20)\n",
    "ax.set_title('Confusion Matrix', fontsize=20)\n",
    "ax.xaxis.set_ticklabels(['Healthy', 'Tumor'], fontsize=20)\n",
    "ax.yaxis.set_ticklabels(['Tumor', 'Healthy'], fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b236a542-30d5-47b8-849a-5fa2b3a255b3",
   "metadata": {},
   "source": [
    "## Entrainer le model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e249326d-3fa5-4edd-9406-d56f6ce07f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.0001\n",
    "EPOCH = 400\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=eta)\n",
    "dataloader = DataLoader(irm_dataset, batch_size=32, shuffle=True)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d25c4-b180-4a7a-b8bf-c99623c62d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCH):\n",
    "    losses = []\n",
    "    for D in dataloader:\n",
    "\n",
    "        optimizer.zero_grad() # Important\n",
    "        \n",
    "        data = D['image'].to(device)\n",
    "        label = D['label'].to(device)\n",
    "        y_hat = model(data)\n",
    "\n",
    "        # Definir la fonction de perte (loss)\n",
    "        error = nn.BCELoss()\n",
    "        loss = torch.sum(error(y_hat.squeeze(), label))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('Train Epoch: {} Loss: {:.6f}'.format(epoch+1, np.mean(losses)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ae936-37e4-4512-877b-8c576b346992",
   "metadata": {},
   "source": [
    "## Evaluation du model après entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55dd381-dc20-4106-afed-847cde9ce863",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dataloader = DataLoader(irm_dataset, batch_size=32, shuffle=False)\n",
    "output = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for D in dataloader:\n",
    "        image = D['image'].to(device)\n",
    "        label = D['label'].to(device)\n",
    "\n",
    "        y_hat = model(image)\n",
    "\n",
    "        output.append(y_hat.cpu().detach().numpy())\n",
    "        y_true.append(label.cpu().detach().numpy())\n",
    "\n",
    "output = np.concatenate(output, axis=0)\n",
    "y_true = np.concatenate(y_true, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9fed06-d2b9-4195-b66b-326facc61bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, threshold(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e56bbcf-4867-438c-98da-45af2d62f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "cm = confusion_matrix(y_true, threshold(output))\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax, annot_kws={\"size\":20})\n",
    "\n",
    "ax.set_xlabel('Predicted labels', fontsize=20)\n",
    "ax.set_ylabel('True labels', fontsize=20)\n",
    "ax.set_title('Confusion Matrix', fontsize=20)\n",
    "ax.xaxis.set_ticklabels(['Healthy', 'Tumor'], fontsize=20)\n",
    "ax.yaxis.set_ticklabels(['Tumor', 'Healthy'], fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa866a10-4cd6-49c3-a1da-ad250eb1b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(output)\n",
    "plt.axvline(x=len(tumor), color='r', linestyle='--')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d62c53-08ff-4baa-b6dc-406b135310ac",
   "metadata": {},
   "source": [
    "## Visualiser utilisant une 'Feature Map'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1c20a-ef9b-4524-bae5-5925356f0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655211e-04f8-4c7c-8438-bc4bf0d298fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_layer = 0\n",
    "conv_layers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f8e4bf-daeb-4373-9ae6-564d38a7fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_children = list(model.children())\n",
    "model_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e4296f-62b7-41ff-9d6b-2fca23b1224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in model_children:\n",
    "    if type(child) == nn.Sequential:\n",
    "        for layer in child.children():\n",
    "            if type(layer) == nn.Conv2d:\n",
    "                no_of_layer += 1\n",
    "                conv_layers.append(layer)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa11d6fd-46fb-4645-b8d3-7d3be21e439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd686d-fb4d-4452-a0dc-e7fa9f72fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = irm_dataset[100]['image']\n",
    "plt.imshow(img.reshape(128,128,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9716ec5-120c-4e8d-88fc-43f9b55a226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.from_numpy(img).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1461775b-80c3-42e5-89fa-f6b168ba900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f61eed-9697-48ee-9a10-4ae9b092b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.unsqueeze(0)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf553cb9-7be3-4397-81df-7f32d051dfe1",
   "metadata": {},
   "source": [
    "## Feature Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb46f602-8b34-4194-adb5-2810df87123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [conv_layers[0](img)]\n",
    "for i in range(1, len(conv_layers)):\n",
    "    results.append(conv_layers[i](results[-1]))\n",
    "output = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f0eee-c896-4f7f-a178-bbd8530e0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e26a1-0353-48ad-9e7d-ffb7b245e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0ec15b-3d97-47dc-8330-244a1e0128f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num_layer in range(len(output)):\n",
    "    plt.figure(figsize=(50,10))\n",
    "    layer_viz = output[num_layer].squeeze()\n",
    "    print(\"Layer \", num_layer+1)\n",
    "    for i, f in enumerate(layer_viz):\n",
    "        plt.subplot(2, 8, i + 1)\n",
    "        plt.imshow(f.detach().cpu().numpy())\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1814af98-6c74-4a92-a12b-d7ffaaf6edfb",
   "metadata": {},
   "source": [
    "## GRAD-CAM (INCOMPLETE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b544e-6679-4f61-a564-e02a16ad1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[52]:\n",
    "def visualize_gradcam(model, dataset, index, device):\n",
    "    \"\"\"\n",
    "    Visualize Grad-CAM for a given image from the dataset using the trained CNN model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained CNN model\n",
    "        dataset: IRM dataset containing images and labels\n",
    "        index: Index of the image in the dataset to visualize\n",
    "        device: Device to run the computation (CPU/GPU/MPS)\n",
    "    \"\"\"\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get the sample from the dataset\n",
    "    sample = dataset[index]\n",
    "    # Convert NumPy array to PyTorch tensor before moving to device\n",
    "    img = torch.from_numpy(sample['image']).to(device).unsqueeze(0)  # Shape: [1, 3, 128, 128]\n",
    "    label = sample['label']  # Ground truth label (0 or 1)\n",
    "    \n",
    "    # Debug: Inspect the image data\n",
    "    print(\"Sample image shape:\", sample['image'].shape)\n",
    "    print(\"Sample image dtype:\", sample['image'].dtype)\n",
    "    print(\"Sample image min/max:\", sample['image'].min(), sample['image'].max())\n",
    "    \n",
    "    # Define the target layer (last convolutional layer: model.cnn_model[3])\n",
    "    target_layer = model.cnn_model[3]\n",
    "    \n",
    "    # Variables to store feature maps and gradients\n",
    "    feature_maps = None\n",
    "    gradients = None\n",
    "    \n",
    "    # Forward hook to capture feature maps\n",
    "    def forward_hook(module, input, output):\n",
    "        nonlocal feature_maps\n",
    "        feature_maps = output.detach()  # Shape: [1, 16, 21, 21]\n",
    "    \n",
    "    # Backward hook to capture gradients\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        nonlocal gradients\n",
    "        gradients = grad_output[0].detach()\n",
    "    \n",
    "    # Register hooks\n",
    "    forward_handle = target_layer.register_forward_hook(forward_hook)\n",
    "    backward_handle = target_layer.register_full_backward_hook(backward_hook)    \n",
    "    \n",
    "    try:\n",
    "        # Forward pass\n",
    "        features = model.cnn_model(img)  # Through convolutional layers\n",
    "        logits = model.fc_model(features.view(features.size(0), -1))  # Through fully connected layers\n",
    "        pred = F.sigmoid(logits)  # Final prediction after sigmoid\n",
    "        \n",
    "        # Backward pass (compute gradients w.r.t. logits for class 1)\n",
    "        model.zero_grad()\n",
    "        target_class = 1  # Class index for tumor\n",
    "        one_hot = torch.zeros_like(logits)\n",
    "        one_hot[0, 0] = 1  # For binary classification with sigmoid\n",
    "        logits.backward(one_hot)\n",
    "        \n",
    "        # Compute Grad-CAM\n",
    "        weights = torch.mean(gradients, dim=[2, 3], keepdim=True)  # Shape: [1, 16, 1, 1]\n",
    "        heatmap = F.relu((weights * feature_maps).sum(dim=1, keepdim=True))  # Shape: [1, 1, 21, 21]\n",
    "        \n",
    "        # Normalize heatmap to [0, 1]\n",
    "        heatmap = heatmap / (heatmap.max() + 1e-10)  # Avoid division by zero\n",
    "        \n",
    "        # Apply a threshold to focus on high-activation regions\n",
    "        threshold = 0.3  # Adjust this value to control focus (higher = more focused)\n",
    "        heatmap = heatmap * (heatmap > threshold).float()  # Zero out values below threshold\n",
    "        \n",
    "        # Renormalize after thresholding to ensure the remaining values are in [0, 1]\n",
    "        if heatmap.max() > 0:  # Avoid division by zero if heatmap is all zeros\n",
    "            heatmap = heatmap / (heatmap.max() + 1e-10)\n",
    "        \n",
    "        # Upsample heatmap to original image size\n",
    "        heatmap = F.interpolate(heatmap, size=(128, 128), mode='bilinear', align_corners=False)  # Shape: [1, 1, 128, 128]\n",
    "        heatmap = heatmap.squeeze().cpu().numpy()  # Shape: [128, 128], convert to NumPy for imshow\n",
    "        \n",
    "        # Apply a Gaussian blur to smooth the heatmap slightly (optional, for better visualization)\n",
    "        heatmap = cv2.GaussianBlur(heatmap, (5, 5), 0)  # Small blur to smooth edges\n",
    "        \n",
    "        # Renormalize after blurring\n",
    "        if heatmap.max() > 0:\n",
    "            heatmap = heatmap / heatmap.max()\n",
    "        \n",
    "        # Prepare original image for visualization\n",
    "        # The image in the dataset is in [3, 128, 128] format and normalized to [0, 1]\n",
    "        # Reshape to [128, 128, 3] without transposing\n",
    "        original_img = sample['image'].reshape(128, 128, 3)  # Reshape directly to [128, 128, 3]\n",
    "        original_img = (original_img * 255).astype(np.uint8)  # Convert to [0, 255] and uint8\n",
    "        \n",
    "        # Debug: Inspect the reshaped image\n",
    "        print(\"Original image shape after reshape:\", original_img.shape)\n",
    "        print(\"Original image dtype after reshape:\", original_img.dtype)\n",
    "        print(\"Original image min/max after reshape:\", original_img.min(), original_img.max())\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Left: Original MRI image (raw, no filters or Grad-CAM)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(original_img)\n",
    "        plt.title(f\"Original MRI Image\\nLabel: {'Tumor' if label == 1 else 'Healthy'}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Right: Grad-CAM overlay on the same raw MRI image\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(original_img)\n",
    "        plt.imshow(heatmap, cmap='jet', alpha=0.5)  # Overlay heatmap with transparency\n",
    "        plt.title(f\"Grad-CAM Visualization\\nPred: {pred.item():.3f}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    finally:\n",
    "        # Remove hooks to prevent memory leaks\n",
    "        forward_handle.remove()\n",
    "        backward_handle.remove()\n",
    "\n",
    "# Example usage: Visualize Grad-CAM for the first tumor image (index 0)\n",
    "visualize_gradcam(model, irm_dataset, 0, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92266c2b-cf36-45e2-858b-07b486bd4f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multiple_gradcam(model, dataset, num_images=10, device=device):\n",
    "    \"\"\"\n",
    "    Visualize Grad-CAM for multiple images from the dataset\n",
    "    \n",
    "    Args:\n",
    "        model: Trained CNN model\n",
    "        dataset: IRM dataset containing images and labels\n",
    "        num_images: Number of images to visualize\n",
    "        device: Device to run computation on\n",
    "    \"\"\"\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get random indices (half tumors, half healthy if possible)\n",
    "    tumor_indices = [i for i, sample in enumerate(dataset) if sample['label'] == 1]\n",
    "    healthy_indices = [i for i, sample in enumerate(dataset) if sample['label'] == 0]\n",
    "    \n",
    "    # Randomly select from each category\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    selected_tumor = np.random.choice(tumor_indices, min(num_images//2, len(tumor_indices)), replace=False)\n",
    "    selected_healthy = np.random.choice(healthy_indices, min(num_images - len(selected_tumor), len(healthy_indices)), replace=False)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    selected_indices = np.concatenate([selected_tumor, selected_healthy])\n",
    "    np.random.shuffle(selected_indices)\n",
    "    \n",
    "    # Limit to requested number\n",
    "    selected_indices = selected_indices[:num_images]\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(15, 5*num_images))\n",
    "    \n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        # Get the sample\n",
    "        sample = dataset[idx]\n",
    "        img = torch.from_numpy(sample['image']).to(device).unsqueeze(0)\n",
    "        label = sample['label']\n",
    "        \n",
    "        # Select target layer\n",
    "        target_layer = model.cnn_model[3]  # Second Conv2d layer\n",
    "        \n",
    "        # Storage for activations and gradients\n",
    "        activations = None\n",
    "        gradients = None\n",
    "        \n",
    "        # Hooks\n",
    "        def forward_hook(module, input, output):\n",
    "            nonlocal activations\n",
    "            activations = output.detach()\n",
    "        \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            nonlocal gradients\n",
    "            gradients = grad_output[0].detach()\n",
    "        \n",
    "        # Register hooks\n",
    "        forward_handle = target_layer.register_forward_hook(forward_hook)\n",
    "        backward_handle = target_layer.register_full_backward_hook(backward_hook)\n",
    "        \n",
    "        try:\n",
    "            # Forward pass\n",
    "            model.zero_grad()\n",
    "            output = model(img)\n",
    "            \n",
    "            # Get predicted class\n",
    "            pred_score = output.item()\n",
    "            pred_class = 1 if pred_score > 0.5 else 0\n",
    "            \n",
    "            # Back-propagate for the predicted class\n",
    "            if pred_class == 1:  # Tumor prediction\n",
    "                output.backward()\n",
    "            else:  # Show what would make it predict tumor\n",
    "                (1 - output).backward()\n",
    "            \n",
    "            # Calculate weights\n",
    "            weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
    "            \n",
    "            # Generate weighted activation map\n",
    "            cam = torch.sum(weights * activations, dim=1, keepdim=True)\n",
    "            cam = F.relu(cam)\n",
    "            \n",
    "            # Normalize\n",
    "            cam = cam - cam.min()\n",
    "            if cam.max() > 0:\n",
    "                cam = cam / cam.max()\n",
    "            \n",
    "            # Resize to input size\n",
    "            cam = F.interpolate(cam, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "            heatmap = cam.squeeze().cpu().numpy()\n",
    "            \n",
    "            # Apply slight blur\n",
    "            heatmap = cv2.GaussianBlur(heatmap, (3, 3), 0)\n",
    "            \n",
    "            # Prepare original image\n",
    "            original_img = sample['image'].reshape(128, 128, 3)\n",
    "            original_img = (original_img * 255).astype(np.uint8)\n",
    "            \n",
    "            # Plot in subplots\n",
    "            # Original\n",
    "            axes[i, 0].imshow(original_img)\n",
    "            axes[i, 0].set_title(f\"Original MRI\\nTrue: {'Tumor' if label == 1 else 'Healthy'}\")\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # Heatmap\n",
    "            axes[i, 1].imshow(heatmap, cmap='jet')\n",
    "            axes[i, 1].set_title(f\"Grad-CAM Heatmap\\nPred: {'Tumor' if pred_class == 1 else 'Healthy'} ({pred_score:.2f})\")\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            # Overlay\n",
    "            axes[i, 2].imshow(original_img)\n",
    "            axes[i, 2].imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "            axes[i, 2].set_title(\"Overlay\")\n",
    "            axes[i, 2].axis('off')\n",
    "            \n",
    "        finally:\n",
    "            # Remove hooks\n",
    "            forward_handle.remove()\n",
    "            backward_handle.remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the function to visualize 10 different MRIs\n",
    "visualize_multiple_gradcam(model, irm_dataset, num_images=10, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (BTD 3.10)",
   "language": "python",
   "name": "btd-3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
